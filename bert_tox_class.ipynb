{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-18T23:42:07.877965Z",
     "iopub.status.busy": "2026-01-18T23:42:07.877664Z",
     "iopub.status.idle": "2026-01-18T23:42:08.992166Z",
     "shell.execute_reply": "2026-01-18T23:42:08.991411Z",
     "shell.execute_reply.started": "2026-01-18T23:42:07.877925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T23:42:08.994124Z",
     "iopub.status.busy": "2026-01-18T23:42:08.993708Z",
     "iopub.status.idle": "2026-01-18T23:42:10.456590Z",
     "shell.execute_reply": "2026-01-18T23:42:10.455999Z",
     "shell.execute_reply.started": "2026-01-18T23:42:08.994100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T23:42:10.457852Z",
     "iopub.status.busy": "2026-01-18T23:42:10.457566Z",
     "iopub.status.idle": "2026-01-18T23:42:10.497656Z",
     "shell.execute_reply": "2026-01-18T23:42:10.497016Z",
     "shell.execute_reply.started": "2026-01-18T23:42:10.457826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, val_df  = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:41:36.598751Z",
     "iopub.status.busy": "2026-01-19T00:41:36.598106Z",
     "iopub.status.idle": "2026-01-19T00:41:36.610595Z",
     "shell.execute_reply": "2026-01-19T00:41:36.609771Z",
     "shell.execute_reply.started": "2026-01-19T00:41:36.598720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from turtle import pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from src.find_optimal_threshold import find_optimal_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T23:42:10.639757Z",
     "iopub.status.busy": "2026-01-18T23:42:10.639496Z",
     "iopub.status.idle": "2026-01-19T00:05:55.378967Z",
     "shell.execute_reply": "2026-01-19T00:05:55.378204Z",
     "shell.execute_reply.started": "2026-01-18T23:42:10.639723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ca52bdd30d44a1be889b69e5955811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ee9e11e9240dab17d93a7e720871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb91414c18948e989ad796e07f5a8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc8c435b9f408eb4776a3d4b1f7195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 23:42:24.866757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768779745.052145      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768779745.109924      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768779745.569433      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768779745.569478      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768779745.569481      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768779745.569483      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6b557c62b54462a265087b6758033d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "BERT + Logistic Regression модель\n",
      "\n",
      "Получение BERT эмбеддингов для обучающего набора...\n",
      "Processed 32/127656 texts\n",
      "Processed 1632/127656 texts\n",
      "Processed 3232/127656 texts\n",
      "Processed 4832/127656 texts\n",
      "Processed 6432/127656 texts\n",
      "Processed 8032/127656 texts\n",
      "Processed 9632/127656 texts\n",
      "Processed 11232/127656 texts\n",
      "Processed 12832/127656 texts\n",
      "Processed 14432/127656 texts\n",
      "Processed 16032/127656 texts\n",
      "Processed 17632/127656 texts\n",
      "Processed 19232/127656 texts\n",
      "Processed 20832/127656 texts\n",
      "Processed 22432/127656 texts\n",
      "Processed 24032/127656 texts\n",
      "Processed 25632/127656 texts\n",
      "Processed 27232/127656 texts\n",
      "Processed 28832/127656 texts\n",
      "Processed 30432/127656 texts\n",
      "Processed 32032/127656 texts\n",
      "Processed 33632/127656 texts\n",
      "Processed 35232/127656 texts\n",
      "Processed 36832/127656 texts\n",
      "Processed 38432/127656 texts\n",
      "Processed 40032/127656 texts\n",
      "Processed 41632/127656 texts\n",
      "Processed 43232/127656 texts\n",
      "Processed 44832/127656 texts\n",
      "Processed 46432/127656 texts\n",
      "Processed 48032/127656 texts\n",
      "Processed 49632/127656 texts\n",
      "Processed 51232/127656 texts\n",
      "Processed 52832/127656 texts\n",
      "Processed 54432/127656 texts\n",
      "Processed 56032/127656 texts\n",
      "Processed 57632/127656 texts\n",
      "Processed 59232/127656 texts\n",
      "Processed 60832/127656 texts\n",
      "Processed 62432/127656 texts\n",
      "Processed 64032/127656 texts\n",
      "Processed 65632/127656 texts\n",
      "Processed 67232/127656 texts\n",
      "Processed 68832/127656 texts\n",
      "Processed 70432/127656 texts\n",
      "Processed 72032/127656 texts\n",
      "Processed 73632/127656 texts\n",
      "Processed 75232/127656 texts\n",
      "Processed 76832/127656 texts\n",
      "Processed 78432/127656 texts\n",
      "Processed 80032/127656 texts\n",
      "Processed 81632/127656 texts\n",
      "Processed 83232/127656 texts\n",
      "Processed 84832/127656 texts\n",
      "Processed 86432/127656 texts\n",
      "Processed 88032/127656 texts\n",
      "Processed 89632/127656 texts\n",
      "Processed 91232/127656 texts\n",
      "Processed 92832/127656 texts\n",
      "Processed 94432/127656 texts\n",
      "Processed 96032/127656 texts\n",
      "Processed 97632/127656 texts\n",
      "Processed 99232/127656 texts\n",
      "Processed 100832/127656 texts\n",
      "Processed 102432/127656 texts\n",
      "Processed 104032/127656 texts\n",
      "Processed 105632/127656 texts\n",
      "Processed 107232/127656 texts\n",
      "Processed 108832/127656 texts\n",
      "Processed 110432/127656 texts\n",
      "Processed 112032/127656 texts\n",
      "Processed 113632/127656 texts\n",
      "Processed 115232/127656 texts\n",
      "Processed 116832/127656 texts\n",
      "Processed 118432/127656 texts\n",
      "Processed 120032/127656 texts\n",
      "Processed 121632/127656 texts\n",
      "Processed 123232/127656 texts\n",
      "Processed 124832/127656 texts\n",
      "Processed 126432/127656 texts\n",
      "\n",
      "Обучение классификатора...\n",
      "\n",
      "Получение BERT эмбеддингов для валидационного набора...\n",
      "Processed 32/31915 texts\n",
      "Processed 1632/31915 texts\n",
      "Processed 3232/31915 texts\n",
      "Processed 4832/31915 texts\n",
      "Processed 6432/31915 texts\n",
      "Processed 8032/31915 texts\n",
      "Processed 9632/31915 texts\n",
      "Processed 11232/31915 texts\n",
      "Processed 12832/31915 texts\n",
      "Processed 14432/31915 texts\n",
      "Processed 16032/31915 texts\n",
      "Processed 17632/31915 texts\n",
      "Processed 19232/31915 texts\n",
      "Processed 20832/31915 texts\n",
      "Processed 22432/31915 texts\n",
      "Processed 24032/31915 texts\n",
      "Processed 25632/31915 texts\n",
      "Processed 27232/31915 texts\n",
      "Processed 28832/31915 texts\n",
      "Processed 30432/31915 texts\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "distilbert_model = distilbert_model.to(device)\n",
    "distilbert_model.eval()\n",
    "\n",
    "def bert_embed(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = distilbert_model(**inputs)\n",
    "        \n",
    "        # CLS токен \n",
    "        batch_embeddings = (outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        # Прогресс\n",
    "        if (i // batch_size) % 50 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} texts\")\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"BERT + Logistic Regression модель\")\n",
    "\n",
    "print(\"\\nПолучение BERT эмбеддингов для обучающего набора...\")\n",
    "X = bert_embed(train_df['comment_text'].tolist(), batch_size=32)\n",
    "\n",
    "print(\"\\nОбучение классификатора...\")\n",
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='liblinear'))\n",
    "clf.fit(X, train_df[TARGET])\n",
    "\n",
    "print(\"\\nПолучение BERT эмбеддингов для валидационного набора...\")\n",
    "X_val = bert_embed(val_df['comment_text'].tolist(), batch_size=32)\n",
    "\n",
    "val_probs = clf.predict_proba(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:41:45.905997Z",
     "iopub.status.busy": "2026-01-19T00:41:45.905405Z",
     "iopub.status.idle": "2026-01-19T00:41:46.049433Z",
     "shell.execute_reply": "2026-01-19T00:41:46.048555Z",
     "shell.execute_reply.started": "2026-01-19T00:41:45.905953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Метрики BERT модели:\n",
      "================================================================================\n",
      "\n",
      "Метрики с оптимальными порогами:\n",
      "--------------------------------\n",
      "F1 Score (macro): 0.6042\n",
      "F1 Score (micro): 0.7132\n",
      "Accuracy: 0.9082\n",
      "Precision: 0.3975\n",
      "Recall: 0.6273\n",
      "--------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76      3056\n",
      "           1       0.42      0.66      0.51       321\n",
      "           2       0.80      0.72      0.76      1715\n",
      "           3       0.46      0.39      0.42        74\n",
      "           4       0.66      0.73      0.70      1614\n",
      "           5       0.45      0.51      0.48       294\n",
      "\n",
      "   micro avg       0.71      0.72      0.71      7074\n",
      "   macro avg       0.59      0.63      0.60      7074\n",
      "weighted avg       0.72      0.72      0.72      7074\n",
      " samples avg       0.06      0.07      0.06      7074\n",
      "\n",
      "\n",
      "Validation ROC-AUC Score (BERT): 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Метрики BERT модели:\")\n",
    "print(\"=\"*80)\n",
    "best_thresholds, best_scores = find_optimal_threshold(val_df[TARGET].values, val_probs)\n",
    "print(\"\\nМетрики с оптимальными порогами:\")\n",
    "metrics_model(val_df[TARGET], val_probs, thresholds=best_thresholds)\n",
    "print(f'\\nValidation ROC-AUC Score (BERT): {roc_auc_score(val_df[TARGET], val_probs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:05:55.572742Z",
     "iopub.status.busy": "2026-01-19T00:05:55.572419Z",
     "iopub.status.idle": "2026-01-19T00:05:56.910344Z",
     "shell.execute_reply": "2026-01-19T00:05:56.909450Z",
     "shell.execute_reply.started": "2026-01-19T00:05:55.572707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:05:56.911632Z",
     "iopub.status.busy": "2026-01-19T00:05:56.911377Z",
     "iopub.status.idle": "2026-01-19T00:23:04.645717Z",
     "shell.execute_reply": "2026-01-19T00:23:04.645048Z",
     "shell.execute_reply.started": "2026-01-19T00:05:56.911609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Получение BERT эмбеддингов для test набора...\n",
      "Processed 32/153164 texts\n",
      "Processed 1632/153164 texts\n",
      "Processed 3232/153164 texts\n",
      "Processed 4832/153164 texts\n",
      "Processed 6432/153164 texts\n",
      "Processed 8032/153164 texts\n",
      "Processed 9632/153164 texts\n",
      "Processed 11232/153164 texts\n",
      "Processed 12832/153164 texts\n",
      "Processed 14432/153164 texts\n",
      "Processed 16032/153164 texts\n",
      "Processed 17632/153164 texts\n",
      "Processed 19232/153164 texts\n",
      "Processed 20832/153164 texts\n",
      "Processed 22432/153164 texts\n",
      "Processed 24032/153164 texts\n",
      "Processed 25632/153164 texts\n",
      "Processed 27232/153164 texts\n",
      "Processed 28832/153164 texts\n",
      "Processed 30432/153164 texts\n",
      "Processed 32032/153164 texts\n",
      "Processed 33632/153164 texts\n",
      "Processed 35232/153164 texts\n",
      "Processed 36832/153164 texts\n",
      "Processed 38432/153164 texts\n",
      "Processed 40032/153164 texts\n",
      "Processed 41632/153164 texts\n",
      "Processed 43232/153164 texts\n",
      "Processed 44832/153164 texts\n",
      "Processed 46432/153164 texts\n",
      "Processed 48032/153164 texts\n",
      "Processed 49632/153164 texts\n",
      "Processed 51232/153164 texts\n",
      "Processed 52832/153164 texts\n",
      "Processed 54432/153164 texts\n",
      "Processed 56032/153164 texts\n",
      "Processed 57632/153164 texts\n",
      "Processed 59232/153164 texts\n",
      "Processed 60832/153164 texts\n",
      "Processed 62432/153164 texts\n",
      "Processed 64032/153164 texts\n",
      "Processed 65632/153164 texts\n",
      "Processed 67232/153164 texts\n",
      "Processed 68832/153164 texts\n",
      "Processed 70432/153164 texts\n",
      "Processed 72032/153164 texts\n",
      "Processed 73632/153164 texts\n",
      "Processed 75232/153164 texts\n",
      "Processed 76832/153164 texts\n",
      "Processed 78432/153164 texts\n",
      "Processed 80032/153164 texts\n",
      "Processed 81632/153164 texts\n",
      "Processed 83232/153164 texts\n",
      "Processed 84832/153164 texts\n",
      "Processed 86432/153164 texts\n",
      "Processed 88032/153164 texts\n",
      "Processed 89632/153164 texts\n",
      "Processed 91232/153164 texts\n",
      "Processed 92832/153164 texts\n",
      "Processed 94432/153164 texts\n",
      "Processed 96032/153164 texts\n",
      "Processed 97632/153164 texts\n",
      "Processed 99232/153164 texts\n",
      "Processed 100832/153164 texts\n",
      "Processed 102432/153164 texts\n",
      "Processed 104032/153164 texts\n",
      "Processed 105632/153164 texts\n",
      "Processed 107232/153164 texts\n",
      "Processed 108832/153164 texts\n",
      "Processed 110432/153164 texts\n",
      "Processed 112032/153164 texts\n",
      "Processed 113632/153164 texts\n",
      "Processed 115232/153164 texts\n",
      "Processed 116832/153164 texts\n",
      "Processed 118432/153164 texts\n",
      "Processed 120032/153164 texts\n",
      "Processed 121632/153164 texts\n",
      "Processed 123232/153164 texts\n",
      "Processed 124832/153164 texts\n",
      "Processed 126432/153164 texts\n",
      "Processed 128032/153164 texts\n",
      "Processed 129632/153164 texts\n",
      "Processed 131232/153164 texts\n",
      "Processed 132832/153164 texts\n",
      "Processed 134432/153164 texts\n",
      "Processed 136032/153164 texts\n",
      "Processed 137632/153164 texts\n",
      "Processed 139232/153164 texts\n",
      "Processed 140832/153164 texts\n",
      "Processed 142432/153164 texts\n",
      "Processed 144032/153164 texts\n",
      "Processed 145632/153164 texts\n",
      "Processed 147232/153164 texts\n",
      "Processed 148832/153164 texts\n",
      "Processed 150432/153164 texts\n",
      "Processed 152032/153164 texts\n",
      "Предсказание вероятностей...\n"
     ]
    }
   ],
   "source": [
    "print(\"Получение BERT эмбеддингов для test набора...\")\n",
    "X_test = bert_embed(test_df[\"comment_text\"].tolist())\n",
    "\n",
    "print(\"Предсказание вероятностей...\")\n",
    "test_probs = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:23:04.647272Z",
     "iopub.status.busy": "2026-01-19T00:23:04.646851Z",
     "iopub.status.idle": "2026-01-19T00:23:05.942400Z",
     "shell.execute_reply": "2026-01-19T00:23:05.941748Z",
     "shell.execute_reply.started": "2026-01-19T00:23:04.647243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.994370</td>\n",
       "      <td>0.272208</td>\n",
       "      <td>0.946440</td>\n",
       "      <td>0.088073</td>\n",
       "      <td>0.939290</td>\n",
       "      <td>0.539226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.002022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.994370      0.272208  0.946440  0.088073  0.939290   \n",
       "1  0000247867823ef7  0.017814      0.000922  0.017004  0.000234  0.005657   \n",
       "2  00013b17ad220c46  0.015578      0.000518  0.009791  0.000008  0.004724   \n",
       "3  00017563c3f7919a  0.000091      0.000004  0.000008  0.000172  0.000030   \n",
       "4  00017695ad8997eb  0.004044      0.000157  0.002619  0.000037  0.000519   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.539226  \n",
       "1       0.000184  \n",
       "2       0.002022  \n",
       "3       0.000002  \n",
       "4       0.000104  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "submission = pd.DataFrame(\n",
    "    test_probs,\n",
    "    columns=TARGET\n",
    ")\n",
    "\n",
    "submission.insert(0, \"id\", test_df[\"id\"].values)\n",
    "\n",
    "\n",
    "submission.to_csv(\n",
    "    \"submission.csv\",\n",
    "    index=False,\n",
    "    float_format=\"%.6f\"\n",
    ")\n",
    "\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:23:05.943909Z",
     "iopub.status.busy": "2026-01-19T00:23:05.943568Z",
     "iopub.status.idle": "2026-01-19T00:23:06.379673Z",
     "shell.execute_reply": "2026-01-19T00:23:06.378807Z",
     "shell.execute_reply.started": "2026-01-19T00:23:05.943866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/model_export/logreg_head.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "\n",
    "EXPORT_DIR = \"/kaggle/working/model_export\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "BERT_DIR = os.path.join(EXPORT_DIR, \"distilbert_embedder\")\n",
    "\n",
    "tokenizer.save_pretrained(BERT_DIR)\n",
    "distilbert_model.save_pretrained(BERT_DIR)\n",
    "joblib.dump(clf, os.path.join(EXPORT_DIR, \"logreg_head.joblib\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 44219,
     "sourceId": 8076,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
