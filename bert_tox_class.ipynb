{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-17T17:34:09.888111Z",
     "iopub.status.busy": "2026-01-17T17:34:09.887092Z",
     "iopub.status.idle": "2026-01-17T17:34:09.897220Z",
     "shell.execute_reply": "2026-01-17T17:34:09.896333Z",
     "shell.execute_reply.started": "2026-01-17T17:34:09.888073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T17:34:09.899223Z",
     "iopub.status.busy": "2026-01-17T17:34:09.898871Z",
     "iopub.status.idle": "2026-01-17T17:34:11.472787Z",
     "shell.execute_reply": "2026-01-17T17:34:11.471993Z",
     "shell.execute_reply.started": "2026-01-17T17:34:09.899196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T17:34:11.474320Z",
     "iopub.status.busy": "2026-01-17T17:34:11.473884Z",
     "iopub.status.idle": "2026-01-17T17:34:11.528178Z",
     "shell.execute_reply": "2026-01-17T17:34:11.527131Z",
     "shell.execute_reply.started": "2026-01-17T17:34:11.474275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, val_df  = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T18:51:35.486838Z",
     "iopub.status.busy": "2026-01-17T18:51:35.486523Z",
     "iopub.status.idle": "2026-01-17T18:51:35.502809Z",
     "shell.execute_reply": "2026-01-17T18:51:35.501780Z",
     "shell.execute_reply.started": "2026-01-17T18:51:35.486812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from turtle import pd\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def metrics_model(y_true, y_prob, thresholds=0.5):\n",
    "    y_pred = applay_thresholds(y_prob, thresholds)\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = average_precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # return f1_macro, accuracy, precision, recall\n",
    "\n",
    "\n",
    "def applay_thresholds(y_prob, thresholds):\n",
    "    if not isinstance(thresholds, list):\n",
    "        y_pred = y_prob > thresholds\n",
    "        return y_pred.astype(int)\n",
    "\n",
    "    y_pred = np.zeros(y_prob.shape)\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        y_pred[:, i] = y_prob[:, i] > threshold\n",
    "    return y_pred.astype(int)\n",
    "\n",
    "\n",
    "def get_top_tox(vectorizer, model, top_k=20):\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    coefs = model.estimators_[0].coef_.flatten()\n",
    "\n",
    "    top_toxic = feature_names[np.argsort(coefs)][-top_k:][::-1]\n",
    "    top_non_toxic = feature_names[np.argsort(coefs)[:top_k]]\n",
    "\n",
    "    print(\"TOXIC WORDS:\")\n",
    "    print(top_toxic)\n",
    "\n",
    "    print(\"\\nNON-TOXIC WORDS:\")\n",
    "    print(top_non_toxic)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.barh(top_toxic, coefs[np.argsort(coefs)][-top_k:][::-1])\n",
    "    plt.xlabel(\"Coefficient Toxicity Value\")\n",
    "    plt.title(\"Top Toxic Words\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # return top_toxic, top_non_toxic, coefs[np.argsort(coefs)][-top_k:][::-1], coefs[np.argsort(coefs)[:top_k]]\n",
    "\n",
    "\n",
    "\n",
    "def explain_text(text, vectorizer, model, top_k=5):\n",
    "    vec = vectorizer.transform([text])\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    coefs = model.estimators_[0].coef_.flatten()\n",
    "\n",
    "    contributions = vec.toarray()[0] * coefs\n",
    "    idx = np.argsort(contributions)[-top_k:]\n",
    "\n",
    "    print( pd.DataFrame({\n",
    "        \"word\": feature_names[idx],\n",
    "        \"contribution\": contributions[idx]\n",
    "    }).sort_values(\"contribution\", ascending=False)\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "def word_contributions(text, vectorizer, model):\n",
    "    clf = model.estimators_[0]\n",
    "\n",
    "    words = text.lower().split()\n",
    "    w = clf.coef_[0]\n",
    "\n",
    "    contributions = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word in vectorizer.model.wv:\n",
    "            vec = vectorizer.model.wv[word]\n",
    "            contributions[word] = float(np.dot(w, vec))\n",
    "\n",
    "    print (dict(\n",
    "        sorted(contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T17:34:11.548241Z",
     "iopub.status.busy": "2026-01-17T17:34:11.547688Z",
     "iopub.status.idle": "2026-01-17T17:34:11.563338Z",
     "shell.execute_reply": "2026-01-17T17:34:11.562383Z",
     "shell.execute_reply.started": "2026-01-17T17:34:11.548212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Найти оптимальный threshold для каждого класса\n",
    "    \n",
    "    metric: 'f1', 'balanced'\n",
    "    \"\"\"\n",
    "\n",
    "    best_thresholds = np.zeros(y_proba.shape[1])\n",
    "    best_scores = np.zeros(y_proba.shape[1])\n",
    "\n",
    "    for i in range(y_proba.shape[1]):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_proba[:, i])\n",
    "    \n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[idx] if idx < len(thresholds) else 0.5\n",
    "        best_score = f1_scores[idx]\n",
    "        \n",
    "        best_thresholds[i] = best_threshold\n",
    "        best_scores[i] = best_score\n",
    "\n",
    "    return best_thresholds.tolist(), best_scores.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T17:34:11.565177Z",
     "iopub.status.busy": "2026-01-17T17:34:11.564663Z",
     "iopub.status.idle": "2026-01-17T18:19:54.365081Z",
     "shell.execute_reply": "2026-01-17T18:19:54.364183Z",
     "shell.execute_reply.started": "2026-01-17T17:34:11.565143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "BERT + Logistic Regression модель\n",
      "================================================================================\n",
      "\n",
      "Получение BERT эмбеддингов для обучающего набора...\n",
      "(Это займет некоторое время, так как используется CPU)\n",
      "Processed 32/127656 texts\n",
      "Processed 1632/127656 texts\n",
      "Processed 3232/127656 texts\n",
      "Processed 4832/127656 texts\n",
      "Processed 6432/127656 texts\n",
      "Processed 8032/127656 texts\n",
      "Processed 9632/127656 texts\n",
      "Processed 11232/127656 texts\n",
      "Processed 12832/127656 texts\n",
      "Processed 14432/127656 texts\n",
      "Processed 16032/127656 texts\n",
      "Processed 17632/127656 texts\n",
      "Processed 19232/127656 texts\n",
      "Processed 20832/127656 texts\n",
      "Processed 22432/127656 texts\n",
      "Processed 24032/127656 texts\n",
      "Processed 25632/127656 texts\n",
      "Processed 27232/127656 texts\n",
      "Processed 28832/127656 texts\n",
      "Processed 30432/127656 texts\n",
      "Processed 32032/127656 texts\n",
      "Processed 33632/127656 texts\n",
      "Processed 35232/127656 texts\n",
      "Processed 36832/127656 texts\n",
      "Processed 38432/127656 texts\n",
      "Processed 40032/127656 texts\n",
      "Processed 41632/127656 texts\n",
      "Processed 43232/127656 texts\n",
      "Processed 44832/127656 texts\n",
      "Processed 46432/127656 texts\n",
      "Processed 48032/127656 texts\n",
      "Processed 49632/127656 texts\n",
      "Processed 51232/127656 texts\n",
      "Processed 52832/127656 texts\n",
      "Processed 54432/127656 texts\n",
      "Processed 56032/127656 texts\n",
      "Processed 57632/127656 texts\n",
      "Processed 59232/127656 texts\n",
      "Processed 60832/127656 texts\n",
      "Processed 62432/127656 texts\n",
      "Processed 64032/127656 texts\n",
      "Processed 65632/127656 texts\n",
      "Processed 67232/127656 texts\n",
      "Processed 68832/127656 texts\n",
      "Processed 70432/127656 texts\n",
      "Processed 72032/127656 texts\n",
      "Processed 73632/127656 texts\n",
      "Processed 75232/127656 texts\n",
      "Processed 76832/127656 texts\n",
      "Processed 78432/127656 texts\n",
      "Processed 80032/127656 texts\n",
      "Processed 81632/127656 texts\n",
      "Processed 83232/127656 texts\n",
      "Processed 84832/127656 texts\n",
      "Processed 86432/127656 texts\n",
      "Processed 88032/127656 texts\n",
      "Processed 89632/127656 texts\n",
      "Processed 91232/127656 texts\n",
      "Processed 92832/127656 texts\n",
      "Processed 94432/127656 texts\n",
      "Processed 96032/127656 texts\n",
      "Processed 97632/127656 texts\n",
      "Processed 99232/127656 texts\n",
      "Processed 100832/127656 texts\n",
      "Processed 102432/127656 texts\n",
      "Processed 104032/127656 texts\n",
      "Processed 105632/127656 texts\n",
      "Processed 107232/127656 texts\n",
      "Processed 108832/127656 texts\n",
      "Processed 110432/127656 texts\n",
      "Processed 112032/127656 texts\n",
      "Processed 113632/127656 texts\n",
      "Processed 115232/127656 texts\n",
      "Processed 116832/127656 texts\n",
      "Processed 118432/127656 texts\n",
      "Processed 120032/127656 texts\n",
      "Processed 121632/127656 texts\n",
      "Processed 123232/127656 texts\n",
      "Processed 124832/127656 texts\n",
      "Processed 126432/127656 texts\n",
      "\n",
      "Обучение классификатора...\n",
      "\n",
      "Получение BERT эмбеддингов для валидационного набора...\n",
      "Processed 32/31915 texts\n",
      "Processed 1632/31915 texts\n",
      "Processed 3232/31915 texts\n",
      "Processed 4832/31915 texts\n",
      "Processed 6432/31915 texts\n",
      "Processed 8032/31915 texts\n",
      "Processed 9632/31915 texts\n",
      "Processed 11232/31915 texts\n",
      "Processed 12832/31915 texts\n",
      "Processed 14432/31915 texts\n",
      "Processed 16032/31915 texts\n",
      "Processed 17632/31915 texts\n",
      "Processed 19232/31915 texts\n",
      "Processed 20832/31915 texts\n",
      "Processed 22432/31915 texts\n",
      "Processed 24032/31915 texts\n",
      "Processed 25632/31915 texts\n",
      "Processed 27232/31915 texts\n",
      "Processed 28832/31915 texts\n",
      "Processed 30432/31915 texts\n",
      "\n",
      "================================================================================\n",
      "Метрики BERT модели:\n",
      "================================================================================\n",
      "\n",
      "Метрики с оптимальными порогами:\n",
      "--------------------------------\n",
      "F1 Score (macro): 0.6039\n",
      "F1 Score (micro): 0.7130\n",
      "Accuracy: 0.9082\n",
      "Precision: 0.3973\n",
      "Recall: 0.6274\n",
      "--------------------------------\n",
      "\n",
      "Validation ROC-AUC Score (BERT): 0.9777\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Использовать CPU (GPU может быть несовместима)\n",
    "# Если хотите использовать GPU: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "distilbert_model = distilbert_model.to(device)\n",
    "distilbert_model.eval()\n",
    "\n",
    "def bert_embed(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = distilbert_model(**inputs)\n",
    "        \n",
    "        # CLS токен \n",
    "        batch_embeddings = (outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        # Прогресс\n",
    "        if (i // batch_size) % 50 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} texts\")\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"BERT + Logistic Regression модель\")\n",
    "\n",
    "print(\"\\nПолучение BERT эмбеддингов для обучающего набора...\")\n",
    "X = bert_embed(train_df['comment_text'].tolist(), batch_size=32)\n",
    "\n",
    "print(\"\\nОбучение классификатора...\")\n",
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='liblinear'))\n",
    "clf.fit(X, train_df[TARGET])\n",
    "\n",
    "print(\"\\nПолучение BERT эмбеддингов для валидационного набора...\")\n",
    "X_val = bert_embed(val_df['comment_text'].tolist(), batch_size=32)\n",
    "\n",
    "val_probs = clf.predict_proba(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T18:52:21.889084Z",
     "iopub.status.busy": "2026-01-17T18:52:21.888148Z",
     "iopub.status.idle": "2026-01-17T18:52:22.084731Z",
     "shell.execute_reply": "2026-01-17T18:52:22.083769Z",
     "shell.execute_reply.started": "2026-01-17T18:52:21.889043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Метрики BERT модели:\n",
      "================================================================================\n",
      "\n",
      "Метрики с оптимальными порогами:\n",
      "--------------------------------\n",
      "F1 Score (macro): 0.6039\n",
      "F1 Score (micro): 0.7130\n",
      "Accuracy: 0.9082\n",
      "Precision: 0.3973\n",
      "Recall: 0.6274\n",
      "--------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      3056\n",
      "           1       0.42      0.66      0.51       321\n",
      "           2       0.80      0.72      0.76      1715\n",
      "           3       0.46      0.39      0.42        74\n",
      "           4       0.66      0.73      0.70      1614\n",
      "           5       0.45      0.51      0.48       294\n",
      "\n",
      "   micro avg       0.71      0.72      0.71      7074\n",
      "   macro avg       0.59      0.63      0.60      7074\n",
      "weighted avg       0.72      0.72      0.72      7074\n",
      " samples avg       0.06      0.07      0.06      7074\n",
      "\n",
      "\n",
      "Validation ROC-AUC Score (BERT): 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Метрики BERT модели:\")\n",
    "print(\"=\"*80)\n",
    "best_thresholds, best_scores = find_optimal_threshold(val_df[TARGET].values, val_probs)\n",
    "print(\"\\nМетрики с оптимальными порогами:\")\n",
    "metrics_model(val_df[TARGET], val_probs, thresholds=best_thresholds)\n",
    "print(f'\\nValidation ROC-AUC Score (BERT): {roc_auc_score(val_df[TARGET], val_probs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 44219,
     "sourceId": 8076,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
